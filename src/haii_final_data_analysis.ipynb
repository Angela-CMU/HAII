{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(adult.metadata)\n",
    "  \n",
    "# variable information \n",
    "# print(adult.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  \n",
       "0          2174             0              40  United-States  \n",
       "1             0             0              13  United-States  \n",
       "2             0             0              40  United-States  \n",
       "3             0             0              40  United-States  \n",
       "4             0             0              40           Cuba  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "<=50K     24720\n",
       "<=50K.    12435\n",
       ">50K       7841\n",
       ">50K.      3846\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "<=50K    37155\n",
       ">50K     11687\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y['income'] = y['income'].str.replace('.', '')\n",
    "# y = y.replace('.', '')\n",
    "# y.value_counts()\n",
    "\n",
    "y_series = y['income'].str.replace('.', '')\n",
    "y_series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    We have 34189 training data\n",
      "    We have 26003 training data whose income are less than or equal to 50K\n",
      "    We have 8186 training data whose income are more than 50K\n",
      "    After upsampling Xy_train_more_then_50K data, we have 52006 training data\n",
      "Confusion matrix:\n",
      " [[10015  1137]\n",
      " [ 1133  2368]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       <=50K     0.8984    0.8980    0.8982     11152\n",
      "        >50K     0.6756    0.6764    0.6760      3501\n",
      "\n",
      "    accuracy                         0.8451     14653\n",
      "   macro avg     0.7870    0.7872    0.7871     14653\n",
      "weighted avg     0.8451    0.8451    0.8451     14653\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn # import scikit-learn\n",
    "from sklearn import preprocessing # import preprocessing utilites\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features_cat = ['workclass', 'education', 'race', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "features_num = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "def build_encoder(X_train):\n",
    "    X_cat = X_train[features_cat]\n",
    "    X_num = X_train[features_num]\n",
    "    \n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    enc.fit(X_cat) # fit the encoder to categories in our data \n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_num)\n",
    "\n",
    "    return enc, scaler\n",
    "\n",
    "def preprocessing_data(X, enc, scaler):\n",
    "    X_cat = X[features_cat]\n",
    "    X_num = X[features_num]\n",
    "\n",
    "    # Pre-processing categorical data using one hot encoding\n",
    "    one_hot = enc.transform(X_cat) # transform data into one hot encoded sparse array format\n",
    "    X_cat_proc = pd.DataFrame(one_hot.toarray(), columns=enc.get_feature_names_out()) # put the newly encoded sparse array back into a pandas dataframe so that we can use it\n",
    "\n",
    "    # Pre-processing (scaling) numerical data\n",
    "    scaled = scaler.transform(X_num)\n",
    "    X_num_proc = pd.DataFrame(scaled, columns=features_num)\n",
    "\n",
    "    X_preprocessed = pd.concat([X_num_proc, X_cat_proc], axis=1, sort=False)\n",
    "    X_preprocessed = X_preprocessed.fillna(0)\n",
    "\n",
    "    return X_preprocessed\n",
    "\n",
    "def upsample(X_train, y_train):\n",
    "    # concatenate X_train and y_train\n",
    "    Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "    print('    We have', Xy_train.shape[0] , 'training data')\n",
    "\n",
    "    # split them into whether the sample is caused by the specific reason\n",
    "    Xy_train_less_than_50K = Xy_train[Xy_train['income'] == '<=50K']\n",
    "    Xy_train_more_then_50K = Xy_train[Xy_train['income'] == '>50K']\n",
    "    print('    We have', Xy_train_less_than_50K.shape[0] , 'training data whose income are less than or equal to 50K')\n",
    "    print('    We have', Xy_train_more_then_50K.shape[0] , 'training data whose income are more than 50K')\n",
    "\n",
    "    # upsample the Xy_train_more_then_50K rows\n",
    "    Xy_train_more_then_50K_up = resample(Xy_train_more_then_50K, n_samples=len(Xy_train_less_than_50K), random_state=1)\n",
    "    Xy_train_up = pd.concat([Xy_train_less_than_50K, Xy_train_more_then_50K_up], axis=0)\n",
    "    print('    After upsampling Xy_train_more_then_50K data, we have', Xy_train_up.shape[0] , 'training data')\n",
    "    X_train_up = Xy_train_up[X_train.columns]\n",
    "    y_train_up = Xy_train_up[y_train.name]\n",
    "\n",
    "    return X_train_up, y_train_up\n",
    "\n",
    "# helper method to print basic model metrics\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "    print('\\nReport:\\n', classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "def train_model(X_train, X_test, y_train, y_test):\n",
    "    # model = LogisticRegression(solver='lbfgs').fit(X_train, y_train) # first fit (train) the model\n",
    "    model = RandomForestClassifier().fit(X_train, y_train) # first fit (train) the model\n",
    "    y_pred = model.predict(X_test) # next get the model's predictions for a sample in the validation set\n",
    "    metrics(y_test, y_pred) # finally evaluate performance\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model(X, y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1) # split out into training 70% of our data\n",
    "    global enc\n",
    "    global scaler\n",
    "    enc, scaler = build_encoder(X_train)\n",
    "    \n",
    "    # # preprocessing then upsample\n",
    "    # # (after preprocessing the X_train_preprocessed is reset, so y_train needs to be reset as well)\n",
    "    # X_train_preprocessed = preprocessing_data(X_train, enc, scaler)\n",
    "    # X_train_up_preprocessed, y_train_up = upsample(X_train_preprocessed, y_train.reset_index(drop=True), death_cause)\n",
    "    \n",
    "    # upsample then preprocessing\n",
    "    X_train_up, y_train_up = upsample(X_train, y_train)\n",
    "    X_train_up_preprocessed = preprocessing_data(X_train_up, enc, scaler)\n",
    "    X_test_preprocessed = preprocessing_data(X_test, enc, scaler)\n",
    "\n",
    "    model = train_model(X_train_up_preprocessed, X_test_preprocessed, y_train_up.reset_index(drop=True), y_test.reset_index(drop=True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(X, y_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05618",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
